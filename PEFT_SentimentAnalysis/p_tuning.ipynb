{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook for p-tuning SST-2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_model, PrefixTuningConfig, TaskType, PromptTuningConfig, PromptTuningInit\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_name_or_path = \"google/t5-v1_1-xl\"\n",
    "tokenizer_name_or_path = \"google/t5-v1_1-xl\"\n",
    "\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_config = PromptTuningConfig(\n",
    "#     task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "#     model_name_or_path=model_name_or_path,\n",
    "#     tokenizer_name_or_path=tokenizer_name_or_path,\n",
    "#     prompt_init=PromptTuningInit.TEXT,\n",
    "#     prompt_tuning_init_text=PromptTuningInit.RANDOM,\n",
    "#     num_virtual_tokens=64\n",
    "# )\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_name_or_path)\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/export/arun/llmenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 25,165,824 || all params: 2,874,923,008 || trainable%: 0.875356450589163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): T5ForConditionalGeneration(\n",
       "    (shared): Embedding(32128, 2048)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 2048)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 32)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-23): 23 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 2048)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 32)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-23): 23 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PrefixEncoder(\n",
       "      (embedding): Embedding(256, 98304)\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(32128, 2048)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating model\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM, \n",
    "    inference_mode=False, \n",
    "    num_virtual_tokens=256\n",
    "    )\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the sst-2 dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'hide new secretions from the parental units ',\n",
       " 'label': 0,\n",
       " 'idx': 0,\n",
       " 'text_label': 'negative'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_column = \"sentence\"\n",
    "label_column = \"text_label\"\n",
    "max_length = 384\n",
    "\n",
    "label_mapping = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "classes = dataset[\"train\"].features[\"label\"].names\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text_label\": [classes[label] for label in x[\"label\"]]},\n",
    "    batched=True,\n",
    "    num_proc=1\n",
    ")\n",
    "\n",
    "## Add a prompt to each sentence\n",
    "# init_prompt = \"\"\"Classify the sentiment of the following sentence as positive or negative.\n",
    "# Sentence: \"\"\"\n",
    "\n",
    "# dataset = dataset.map(\n",
    "#     lambda x: {\"input_text\": [(init_prompt + text).strip() for text in x[\"sentence\"]]},\n",
    "#     batched=True,\n",
    "#     num_proc=1\n",
    "# )\n",
    "\n",
    "dataset[\"train\"][0]\n",
    "# print(dataset[\"train\"].features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fd3b46eec64df0a64fa0b490b42422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08842e5f04704238ac1b74a7cd0aadc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33eb90f534a4ffba27f65e1fdf0f9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[text_column]\n",
    "    targets = examples[label_column]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    labels = tokenizer(targets, max_length=2, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    labels = labels[\"input_ids\"]\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "processed_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]\n",
    "eval_dataset = processed_datasets[\"validation\"]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and lr scheduler\n",
    "lr = 1e-4\n",
    "num_epochs = 2\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "model.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54927706c2c40dd983d84b151320dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">epoch: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16.147927101207742</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "epoch: \u001b[1;36m0\u001b[0m loss: \u001b[1;36m16.147927101207742\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/mnt/export/arun/llmenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: \n",
       "`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you\n",
       "want to force a new download, use `force_download=True`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/mnt/export/arun/llmenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: \n",
       "`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you\n",
       "want to force a new download, use `force_download=True`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">epoch: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.556170704359113</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "epoch: \u001b[1;36m1\u001b[0m loss: \u001b[1;36m11.556170704359113\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.progress import Progress\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "with Progress() as progress:\n",
    "    task = progress.add_task(\"[red]Training...\", total=num_epochs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_task = progress.add_task(f\"Epoch {epoch}\", total=len(train_dataloader))\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # print(batch[\"input_ids\"].shape)\n",
    "            # print(batch[\"labels\"].shape)\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            labels = batch[\"labels\"]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            losses.append(loss.item())\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()        \n",
    "            progress.update(epoch_task, advance=1)\n",
    "            # progress.print(f\"loss: {loss.item()}\", end=\"\\r\")\n",
    "\n",
    "        test_task = progress.add_task(f\"Epoch {epoch}\", total=len(eval_dataloader))\n",
    "        model.eval()\n",
    "        for batch in eval_dataloader:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            progress.update(test_task, advance=1)\n",
    "            # progress.print(f\"eval_loss: {loss.item()}\", end=\"\\r\")\n",
    "            \n",
    "\n",
    "        progress.update(task, advance=1)\n",
    "        progress.print(f\"epoch: {epoch} loss: {sum(losses) / len(losses)}\")\n",
    "        model.save_pretrained(\"t5-efficient-xxl-prefix-tuning-sst2-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"t5-efficient-xxl-prefix-tuning-sst2-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/export/arun/llmenv/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test on 1 sample\n",
    "\n",
    "def generate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs)\n",
    "        \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "input = \"\"\"I Love this movie\"\"\"\n",
    "output = generate_text(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "pred = []\n",
    "true = []\n",
    "for item in tqdm(dataset[\"validation\"]):\n",
    "    prompt = item[\"sentence\"]\n",
    "    output = generate_text(prompt)\n",
    "    pred.append(output)\n",
    "    true.append(item[\"text_label\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(pred)\n",
    "\n",
    "# pred = [p.lower() for p in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "\n",
    "pred = []\n",
    "true = []\n",
    "\n",
    "for idx, item in tqdm(enumerate(imdb_dataset[\"test\"])):\n",
    "    text = item[\"text\"]\n",
    "    label = label_mapping[item[\"label\"]]\n",
    "    output = generate_text(text)\n",
    "    pred.append(output)\n",
    "    true.append(label)\n",
    "\n",
    "    if idx == 1000:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SST-2 VAL SET\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMDB TEST SET\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_phrasebank = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\n",
    "financial_phrasebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_phrasebank[\"train\"][0]\n",
    "\n",
    "for idx, item in tqdm(enumerate(financial_phrasebank[\"train\"])):\n",
    "    text = item[\"sentence\"]\n",
    "    if item[\"label\"] != 2:\n",
    "        label = label_mapping[item[\"label\"]]\n",
    "        output = generate_text(text)\n",
    "        pred.append(output)\n",
    "        true.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = []\n",
    "pred = []\n",
    "\n",
    "validation_output = {}\n",
    "validation_output[\"sentence\"] = []\n",
    "validation_output[\"true\"] = []\n",
    "validation_output[\"pred\"] = []\n",
    "for item in dataset[\"validation\"]:\n",
    "    input = item[\"sentence\"]\n",
    "    target = item[\"text_label\"]\n",
    "    output = generate_text(input)\n",
    "    validation_output[\"sentence\"].append(input)\n",
    "    validation_output[\"true\"].append(target)\n",
    "    validation_output[\"pred\"].append(output)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(validation_output)\n",
    "\n",
    "df.to_csv(\"t5-large-prefix-tuning-sst2_validation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(true, pred, labels=[\"positive\", \"negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy -- Accuracy on the Validation Set\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_output = {}\n",
    "test_set_output[\"sentence\"] = []\n",
    "test_set_output[\"predicted\"] = []\n",
    "\n",
    "for item in dataset[\"test\"]:\n",
    "    input = item[\"sentence\"]\n",
    "    output = generate_text(input)\n",
    "    test_set_output[\"sentence\"].append(input)\n",
    "    test_set_output[\"predicted\"].append(output)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(test_set_output)\n",
    "df.to_csv(\"test_set_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    if row[\"true\"] != row[\"pred\"]:\n",
    "        print(row[\"sentence\"])\n",
    "        print(f\"true: {row['true']}, predicted: {row['pred']}\")\n",
    "        print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Base Model Accuracy\n",
    "from tqdm import tqdm\n",
    "device = \"cuda\"\n",
    "model_name_or_path = \"google/flan-t5-large\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name_or_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name_or_path)\n",
    "model.to(device)\n",
    "\n",
    "def generate_text(text):\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=2, do_sample=False)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# true = []\n",
    "# pred = []\n",
    "# for item in tqdm(dataset[\"validation\"]):\n",
    "#     input = item[\"sentence\"]\n",
    "#     target = item[\"text_label\"]\n",
    "#     output = generate_text(input)\n",
    "#     true.append(target)\n",
    "#     pred.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "prompt_template = \"\"\"Classify the sentiment of the given sentence as positive or negative \n",
    "sentence: {sentence} \n",
    "Answer: \"\"\"\n",
    "\n",
    "pred = []\n",
    "true = []\n",
    "\n",
    "for item in tqdm(dataset[\"validation\"]):\n",
    "    text = item[\"sentence\"]\n",
    "    prompt = prompt_template.format(sentence=text)\n",
    "    output = generate_text(prompt)\n",
    "    pred.append(output)\n",
    "    true.append(item[\"text_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dataset[\"validation\"][0][\"sentence\"]\n",
    "prompt = prompt_template.format(sentence=text)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(**input)\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map \"Fal\" to \"negative\" and \"Tru\" to \"positive\"\n",
    "\n",
    "label_mapping = {\n",
    "    \"Fal\": \"negative\",\n",
    "    \"True\": \"positive\"\n",
    "}\n",
    "\n",
    "pred = [label_mapping[item] for item in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import deepspeed\n",
    "# from rich.progress import Progress\n",
    "# from rich.console import Console\n",
    "\n",
    "# console = Console()\n",
    "\n",
    "# # Import DeepSpeed and add configuration\n",
    "# ds_config = \"ds_config.json\"  # Path to the DeepSpeed config file\n",
    "\n",
    "# # Wrap the model with DeepSpeed\n",
    "# model, optimizer, _, lr_scheduler = deepspeed.initialize(\n",
    "#     model=model,\n",
    "#     optimizer=optimizer,\n",
    "#     lr_scheduler=lr_scheduler,\n",
    "#     config=ds_config,\n",
    "#     model_parameters=model.parameters()\n",
    "# )\n",
    "\n",
    "# with Progress() as progress:\n",
    "#     train_task = progress.add_task(\"[red]Training...\", total=num_epochs)\n",
    "#     for epoch in range(num_epochs):\n",
    "#         epoch_task = progress.add_task(f\"Epoch {epoch + 1}/{num_epochs}\", total=len(train_dataloader))\n",
    "#         model.train()\n",
    "        \n",
    "#         for step, batch in enumerate(train_dataloader):\n",
    "#             batch = {k: v.to(model.device) for k, v in batch.items()}  # Move batch to the correct device\n",
    "#             outputs = model(**batch)\n",
    "#             loss = outputs.loss\n",
    "#             model.backward(loss)   # DeepSpeed manages backward and optimizer step\n",
    "#             model.step()           # Executes DeepSpeed's step\n",
    "            \n",
    "#             progress.update(epoch_task, advance=1)\n",
    "\n",
    "#         # Evaluation step\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for step, batch in enumerate(eval_dataloader):\n",
    "#                 batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "#                 outputs = model(**batch)\n",
    "#                 eval_loss = outputs.loss\n",
    "#                 progress.update(train_task, advance=1)\n",
    "        \n",
    "#         progress.update(train_task, completed=num_epochs)\n",
    "#         model.save_checkpoint(\"sst2-t5-xl-prefix-tuning\", tag=f\"epoch_{epoch}\")\n",
    "\n",
    "#         console.log(f\"Epoch: {epoch} -- Train Loss: {loss.item()} -- Eval Loss: {eval_loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
