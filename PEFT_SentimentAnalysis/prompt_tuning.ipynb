{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Notebook for finetuning a model on SST-2 Dataset with Prompt Tuning PEFT method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from peft import get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_name_or_path = \"google/flan-t5-xl\"\n",
    "tokenizer_name_or_path = \"google/flan-t5-xl\"\n",
    "\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the Tokenizer and Model\n",
    "tokenizer = T5Tokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name_or_path)\n",
    "\n",
    "## Define the Prompt Tuning Configuration\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=200,\n",
    "    prompt_tuning_init_text=PromptTuningInit.RANDOM,\n",
    "    inference_mode=False,\n",
    "    tokenizer_name_or_path=tokenizer_name_or_path\n",
    ")\n",
    "\n",
    "## Get the Prompt Tuning Model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the sst-2 dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = \"sentence\"\n",
    "label_column = \"text_label\"\n",
    "max_length = 256\n",
    "\n",
    "label_mapping = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "classes = dataset[\"train\"].features[\"label\"].names\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text_label\": [classes[label] for label in x[\"label\"]]},\n",
    "    batched=True,\n",
    "    num_proc=1\n",
    ")\n",
    "\n",
    "dataset[\"train\"][0]\n",
    "# print(dataset[\"train\"].features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples[text_column]\n",
    "    targets = examples[label_column]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    labels = tokenizer(targets, max_length=2, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    labels = labels[\"input_ids\"]\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "processed_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]\n",
    "eval_dataset = processed_datasets[\"validation\"]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and lr scheduler\n",
    "lr = 1e-4\n",
    "num_epochs = 2\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.progress import Progress\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "with Progress() as progress:\n",
    "    task = progress.add_task(\"[red]Training...\", total=num_epochs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_task = progress.add_task(f\"Epoch {epoch}\", total=len(train_dataloader))\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            losses.append(loss.item())\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()        \n",
    "            progress.update(epoch_task, advance=1)\n",
    "            # progress.print(f\"loss: {loss.item()}\", end=\"\\r\")\n",
    "\n",
    "        test_task = progress.add_task(f\"Epoch {epoch}\", total=len(eval_dataloader))\n",
    "        model.eval()\n",
    "        for batch in eval_dataloader:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            progress.update(test_task, advance=1)\n",
    "            # progress.print(f\"eval_loss: {loss.item()}\", end=\"\\r\")\n",
    "            \n",
    "\n",
    "        progress.update(task, advance=1)\n",
    "        progress.print(f\"epoch: {epoch} loss: {sum(losses) / len(losses)}\")\n",
    "        model.save_pretrained(f\"tf-xl-prompt-tuning-sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Evaluate the model\n",
    "\n",
    "# model.eval()\n",
    "# eval_loss = 0\n",
    "# predictions = []\n",
    "# true = []\n",
    "\n",
    "# for step, batch in enumerate(eval_dataloader):\n",
    "#     input_ids, attention_mask, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs.loss\n",
    "#         eval_loss += loss.item()\n",
    "#         logits = outputs.logits\n",
    "#         predictions.extend(logits.argmax(-1).tolist())\n",
    "#         true.extend(labels.tolist())\n",
    "\n",
    "# eval_loss = eval_loss / len(eval_dataloader)\n",
    "# print(f\"Evaluation Loss: {eval_loss}\")\n",
    "\n",
    "# ## Calculate the Accuracy\n",
    "# correct = 0\n",
    "# total = 0\n",
    "\n",
    "# for p, t in zip(predictions, true):\n",
    "#     if p == t:\n",
    "#         correct += 1\n",
    "#     total += 1\n",
    "\n",
    "# accuracy = correct / total\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# ## Confusion Matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# label_mappping = {0: \"Negative\", 1: \"Positive\"}\n",
    "# predictions = [label_mappping[p] for p in predictions]\n",
    "# true = [label_mappping[t] for t in true]\n",
    "\n",
    "# cm = confusion_matrix(true, predictions)\n",
    "# sns.heatmap(cm, annot=True, xticklabels=label_mappping.values(), yticklabels=label_mappping.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(eval_dataloader):\n",
    "    input_ids, attention_mask, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.item()\n",
    "        logits = outputs.logits\n",
    "        print(outputs.logits.argmax(-1))\n",
    "        print(tokenizer.decode(outputs.logits.argmax(-1)[0]))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
